---
title: "PHP 2550 HW 6"
author: "Blain Morin"
date: "December 2, 2018"
output: pdf_document
header-includes:
- \usepackage{float}
---


```{r, echo = FALSE, message = FALSE, warning = FALSE}

### Load Libraries

library(knitr)
library(readr)
library(ggplot2)
library(dplyr)
library(stargazer)
library(splines)
library(gam)
library(extrafont)
library(grid)
library(gridExtra)
library(caret)

```

```{r, echo = FALSE, message = FALSE, warning = FALSE}

### Load and Clean Data procedure from hw5

### Load data
iod = read_csv("iodatadev.csv")

### Filter out columns with more that 10% missing data
iod.clean = Filter(function(x) mean(is.na(x)) < 0.1, iod)

### Get complete cases
iod.clean = iod.clean %>%
  filter(complete.cases(.))

### Remove ids, collinear terms, and unknown variables 
iod.clean = iod.clean %>%
  select(-X1, -ID, -"..2", -"..2_1", -CSG, -drds, - rass1)

### Select the variables from homework 5
iod.clean = iod.clean %>%
  select(GFR, SUN, SCR, AGE, FEMALE, cys, Diabetes, BMI, BLACK)
  

```

```{r, echo = FALSE, fig.height=5, fig.width=8, fig.align='center'}

### First consider log transforms of continuous variables

### Use log GFR?

gfr.qq = iod.clean %>%
  ggplot(aes(sample = scale(GFR))) +
  stat_qq() + stat_qq_line() +
  stat_qq(aes(sample = scale(log(GFR))), color = "blue") +
  ylab("Sample Quantile") + 
  xlab("Theoretical Quantile") +
  ggtitle("qqPlot: GFR (Black) vs log GFR (Blue)") +
  theme_classic() +
  theme(text=element_text(size=12,  family="CM Sans"))
  

gfr.qq


```


```{r, echo = FALSE, fig.width=12, fig.height=12}

sun.smooth = iod.clean %>%
  ggplot(aes(y = log(GFR), x = SUN)) +
  geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE) +
  ylab("log(GFR)") +
  xlab("SUN") +
  ggtitle("log(GFR) vs SUN") +
  theme_classic() +
  theme(text=element_text(size=12,  family="CM Sans"))

scr.smooth = iod.clean %>%
  ggplot(aes(y = log(GFR), x = SCR)) +
  geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE) +
  ylab("log(GFR)") +
  xlab("SCR") +
  ggtitle("log(GFR) vs SCR") +
  theme_classic() +
  theme(text=element_text(size=12,  family="CM Sans"))

age.smooth = iod.clean %>%
  ggplot(aes(y = log(GFR), x = AGE)) +
  geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE) +
  ylab("log(GFR)") +
  xlab("AGE") +
  ggtitle("log(GFR) vs AGE") +
  theme_classic() +
  theme(text=element_text(size=12,  family="CM Sans"))

cys.smooth = iod.clean %>%
  ggplot(aes(y = log(GFR), x = cys)) +
  geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE) +
  ylab("log(GFR)") +
  xlab("CYS") +
  ggtitle("log(GFR) vs CYS") +
  theme_classic() +
  theme(text=element_text(size=12,  family="CM Sans"))

bmi.smooth = iod.clean %>%
  ggplot(aes(y = log(GFR), x = BMI)) +
  geom_point() +
  geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se = FALSE) +
  ylab("log(GFR)") +
  xlab("BMI") +
  ggtitle("log(GFR) vs BMI") +
  theme_classic() +
  theme(text=element_text(size=12,  family="CM Sans"))

grid.arrange(sun.smooth, scr.smooth,
             age.smooth, cys.smooth,
             bmi.smooth,
             nrow = 3)
  


```


```{r, echo = FALSE}


### Crossvalidation for best polynomial 


bestpoly = function(x, y, folds, maxpoly){
  
  leaveout = createFolds(x, k = folds)
  error.matrix = matrix(NA, nrow = folds, ncol = maxpoly)
  
  for (i in 1:folds) {
    
    trainx = x[-leaveout[[i]]]
    trainy = y[-leaveout[[i]]]
    
    testx = x[leaveout[[i]]]
    testy = y[leaveout[[i]]]
    
    for (j in 1:maxpoly) {
      
      mod = lm(trainy ~ poly(trainx, degree = j, raw = TRUE))
      preds = predict(mod, newdata = poly(testx, degree = j))
      error = mean((testy - preds)^2)
      error.matrix[i,j] = error
      
    }
    
  }
  
  ave.mse = apply(error.matrix, 2, mean)
  
  return(ave.mse)
  
  
}


bestpoly(x = iod.clean$AGE, y = log(iod.clean$GFR), folds = 10, maxpoly = 6)



x = iod.clean$AGE
y = log(iod.clean$GFR)

leaveout = createFolds(x, k = 10)

xtrain = x[-leaveout[[1]]]
ytrain = y[-leaveout[[1]]]

testx = x[leaveout[[1]]]
testy = y[leaveout[[1]]]

mod = lm(ytrain ~ poly(xtrain, degree = 2, raw = TRUE))

preds = predict(mod, newdata = (poly(testx, degree = 2, raw = TRUE, coefs = coef(mod))))

mean((testy - preds)^2)

```

